{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from Modules import AdaBoost, KNN, DecisionTree, PolyLogisticRegression, LogisticRegression, MLP, RandomForest, SVM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Importing dataset. \n",
    "We can try differnt preprocessed datasets here (Ex. PCA, SVD and etc).\n",
    "Please comment out other datasets except the desired one. \n",
    "\"\"\"\n",
    "# original\n",
    "train = pd.read_csv('../Data/adult_train_preprocessed.csv')\n",
    "test = pd.read_csv('../Data/adult_test_preprocessed.csv')\n",
    "\n",
    "# # PCA Variance\n",
    "# train = pd.read_csv('../Data/train_PCA_variance.csv')\n",
    "# test = pd.read_csv('../Data/test_PCA_variance.csv')\n",
    "\n",
    "# # PCA Manual\n",
    "# train = pd.read_csv('../Data/train_PCA_manual.csv')\n",
    "# test = pd.read_csv('../Data/test_PCA_manual.csv')\n",
    "\n",
    "# # Factor Analysis\n",
    "# train = pd.read_csv('../Data/train_FA.csv')\n",
    "# test = pd.read_csv('../Data/test_FA.csv')\n",
    "\n",
    "# # SVD\n",
    "# train = pd.read_csv('../Data/train_SVD.csv')\n",
    "# test = pd.read_csv('../Data/test_SVD.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ab': <Modules.AdaBoost.AB_model object at 0x1a21636410>, 'dt': <Modules.DecisionTree.DT_model object at 0x1a215e6d10>, 'knn': <Modules.KNN.KNN_model object at 0x1a2160aa90>, 'lr': <Modules.LogisticRegression.LR_model object at 0x1a21636510>, 'plr': <Modules.PolyLogisticRegression.PLR_model object at 0x1a21636550>, 'mlp': <Modules.MLP.MLP_model object at 0x1a21636590>, 'rf': <Modules.RandomForest.RF_model object at 0x1a216365d0>, 'svm': <Modules.SVM.SVM_model object at 0x1a1fdba610>}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Make model instances here.\n",
    "\"\"\"\n",
    "ab = AdaBoost.AB_model(train,test,flatten=False)\n",
    "dt = DecisionTree.DT_model(train,test,flatten=False)\n",
    "knn = KNN.KNN_model(train,test,flatten=False)\n",
    "lr = LogisticRegression.LR_model(train,test,flatten=False)\n",
    "plr = PolyLogisticRegression.PLR_model(train,test,flatten=False)\n",
    "mlp = MLP.MLP_model(train,test,flatten=False)\n",
    "rf = RandomForest.RF_model(train,test,flatten=False)\n",
    "svm = SVM.SVM_model(train,test,prob=True,flatten=False)\n",
    "\n",
    "models = {'ab':ab,'dt':dt,'knn':knn,'lr':lr,'plr':plr,'mlp':mlp,'rf':rf,'svm':svm} # all models\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================================================================\n",
      "ab: {'accuracy': 0.8606241699867198, 'precision': 0.7601559961000975, 'recall': 0.6321621621621621, 'f1_score': 0.6902759333038218}\n",
      "best_param: {'learning_rate': 1, 'n_estimators': 200}\n",
      "ab took 23.52608370780945 seconds\n",
      "==============================================================================================================================\n",
      "dt: {'accuracy': 0.8529880478087649, 'precision': 0.7686189443239335, 'recall': 0.5745945945945946, 'f1_score': 0.6575935663470462}\n",
      "best_param: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "dt took 0.5689668655395508 seconds\n",
      "==============================================================================================================================\n",
      "knn: {'accuracy': 0.8387782204515273, 'precision': 0.7172131147540983, 'recall': 0.5675675675675675, 'f1_score': 0.6336753168376583}\n",
      "best_param: {'n_neighbors': 20}\n",
      "knn took 50.72565197944641 seconds\n",
      "==============================================================================================================================\n",
      "lr: {'accuracy': 0.8440239043824701, 'precision': 0.7252417472490831, 'recall': 0.5878378378378378, 'f1_score': 0.6493506493506495}\n",
      "best_param: {'C': 0.8, 'max_iter': 10000}\n",
      "lr took 2.2353689670562744 seconds\n",
      "==============================================================================================================================\n",
      "plr: {'accuracy': 0.8487383798140771, 'precision': 0.7210820895522388, 'recall': 0.6267567567567568, 'f1_score': 0.6706188548293812}\n",
      "best_param: {'C': 0.6, 'max_iter': 10000}\n",
      "plr took 48.662585735321045 seconds\n",
      "==============================================================================================================================\n",
      "mlp: {'accuracy': 0.8519920318725099, 'precision': 0.7542343587970964, 'recall': 0.5897297297297297, 'f1_score': 0.661914151372668}\n",
      "best_param: {'activation': 'tanh', 'alpha': 0.001, 'batch_size': 256, 'hidden_layer_sizes': (64, 32, 16, 8), 'learning_rate_init': 0.001}\n",
      "mlp took 30.47713828086853 seconds\n",
      "==============================================================================================================================\n",
      "rf: {'accuracy': 0.8583665338645419, 'precision': 0.7734729493891798, 'recall': 0.5989189189189189, 'f1_score': 0.6750952018278751}\n",
      "best_param: {'criterion': 'entropy', 'max_depth': 15, 'max_features': 10, 'min_samples_split': 2}\n",
      "rf took 92.89529585838318 seconds\n",
      "==============================================================================================================================\n",
      "svm: {'accuracy': 0.850929614873838, 'precision': 0.745694022289767, 'recall': 0.5967567567567568, 'f1_score': 0.6629635189911425}\n",
      "best_param: {'coef0': 0.0, 'kernel': 'rbf'}\n",
      "svm took 752.3746411800385 seconds\n",
      "CPU times: user 16min 52s, sys: 15 s, total: 17min 6s\n",
      "Wall time: 16min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Find the optimal models and print test scores\n",
    "params = {}\n",
    "results = {}\n",
    "\n",
    "\"\"\"\n",
    "Optimal params for default dataset.\n",
    "\"\"\"\n",
    "params['mlp'] = {'hidden_layer_sizes':[(64,32,16,8)],\n",
    "                'learning_rate_init':[0.001],\n",
    "                'activation':['tanh'],\n",
    "                'alpha':[0.001],\n",
    "                'batch_size':[256]}\n",
    "params['lr'] = {\"C\":[0.8], \"max_iter\":[10000]} # default solver lbfgs only supports l2 penalties according to sklearn doc\n",
    "params['plr'] = {\"C\":[0.6], \"max_iter\":[10000]} # default solver lbfgs only supports l2 penalties according to sklearn doc\n",
    "params['rf']={'max_depth': [15],\n",
    "              'criterion': [\"entropy\"],\n",
    "              'min_samples_split': [2],\n",
    "              'max_features': [10]}\n",
    "params['svm'] = {\"coef0\": [0.0],\n",
    "                 \"kernel\":['rbf']}\n",
    "params['ab'] = {'n_estimators':[200], 'learning_rate':[1]}\n",
    "params['knn'] = {'n_neighbors':[20] }\n",
    "params['dt'] = {'criterion': ['entropy'], 'max_depth': [10], 'min_samples_split': [2], 'splitter': ['best']}\n",
    "\n",
    "for k in models.keys():\n",
    "    start = time.time()\n",
    "    models[k].optimize_model(params[k])\n",
    "    models[k].get_test_score()\n",
    "    results[k] = models[k].test_score\n",
    "    end = time.time()\n",
    "    print('==============================================================================================================================')\n",
    "    print(f'{k}: {results[k]}')\n",
    "    print(f'best_param: {models[k].best_param}')\n",
    "    print(f'{k} took {end-start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models....\n",
      "The order of models are : ['ab', 'dt', 'knn', 'lr', 'plr', 'mlp', 'rf', 'svm']\n",
      "Their accuracies are : [0.8606241699867198, 0.8529880478087649, 0.8387782204515273, 0.8440239043824701, 0.8487383798140771, 0.8519920318725099, 0.8583665338645419, 0.850929614873838]\n",
      "Thus, the weights are : [0.12644261 0.12532072 0.12323301 0.12400371 0.12469636 0.12517438\n",
      " 0.12611092 0.12501829]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Get a list of weights based on accuracy. Can be any other scores.\n",
    "\"\"\"\n",
    "\n",
    "accuracy_list = [v['accuracy'] for k,v in results.items()]\n",
    "weights = np.array(accuracy_list)/np.sum(accuracy_list)\n",
    "print('All models....')\n",
    "print(f'The order of models are : {[k for k in results.keys()]}')\n",
    "print(f'Their accuracies are : {accuracy_list}')\n",
    "print(f'Thus, the weights are : {weights}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Ensemble method (VotingClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ab', AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1,\n",
      "                   n_estimators=200, random_state=None)), ('dt', DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
      "                       max_depth=10, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')), ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=20, p=2,\n",
      "                     weights='uniform')), ('lr', LogisticRegression(C=0.8, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)), ('plr', LogisticRegression(C=0.6, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)), ('mlp', MLPClassifier(activation='tanh', alpha=0.001, batch_size=256, beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(64, 32, 16, 8), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
      "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.5, random_state=123, shuffle=True, solver='adam',\n",
      "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "              warm_start=False)), ('rf', RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='entropy', max_depth=15, max_features=10,\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=123,\n",
      "                       verbose=0, warm_start=False)), ('svm', SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=2, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=True, random_state=123, shrinking=True, tol=0.001,\n",
      "    verbose=False))]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "\n",
    "best_estimators = [(k,models[k].best_model) for k in models.keys()]\n",
    "X_train = train.values[:,:-1]\n",
    "y_train = train.values[:,-1]\n",
    "X_test = test.values[:,:-1]\n",
    "y_test = test.values[:,-1].astype(int)\n",
    "ensemble_models = {}\n",
    "ensemble_preds = {}\n",
    "ensemble_scores = {}\n",
    "\n",
    "print(best_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================================================\n",
      "soft_uniform result: {'accuracy': 0.8567065073041169, 'precision': 0.7695804195804196, 'recall': 0.5948648648648649, 'f1_score': 0.6710365853658536}\n",
      "soft_uniform took 203.2453351020813 seconds\n",
      "CPU times: user 20 s, sys: 551 ms, total: 20.6 s\n",
      "Wall time: 3min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\"\n",
    "Final ensemble model - soft voting with uniform weight.\n",
    "\"\"\"\n",
    "\n",
    "model_name = 'soft_uniform'\n",
    "start = time.time()\n",
    "ensemble_models[model_name] = VotingClassifier(estimators=best_estimators,voting='soft',weights=None,n_jobs=-1)\n",
    "ensemble_models[model_name] = ensemble_models[model_name].fit(X_train,y_train)\n",
    "ensemble_preds[model_name] = ensemble_models[model_name].predict(X_test)\n",
    "ensemble_scores[model_name] = {'accuracy':accuracy_score(y_test,ensemble_preds[model_name]),\n",
    "                               'precision':precision_score(y_test,ensemble_preds[model_name]),\n",
    "                               'recall':recall_score(y_test,ensemble_preds[model_name]),\n",
    "                               'f1_score':f1_score(y_test,ensemble_preds[model_name])}\n",
    "end = time.time()\n",
    "print('===============================================================================================================')\n",
    "print(f'{model_name} result: {ensemble_scores[model_name]}')\n",
    "print(f'{model_name} took {end-start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
